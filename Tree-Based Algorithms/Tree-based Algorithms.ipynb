{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hB8uQCzVs6VI"
   },
   "source": [
    "# Spring 2019 CX4240 Homework 4\n",
    "\n",
    "## Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: Tuesday July 23, 2019, 11:59 pm\n",
    "\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged, but each student must write his own answers and explicitly mention any collaborators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submited by: Huy Thong Nguyen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7U0WVt07tGRv"
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0Ui6T2as9iI"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdkkppO-pNEn"
   },
   "source": [
    "## Part 1: Utility Functions [50pts]\n",
    "\n",
    "Here, we ask you to develop a few functions that will be the main building blocks of your decision tree and random forest algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7e_A6GBe11nA"
   },
   "source": [
    "### Entropy and information gain [20pts]\n",
    "\n",
    "First, we computes entropy and then use this entropy for information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVcXl9-D2DMW"
   },
   "outputs": [],
   "source": [
    "def entropy(class_y):\n",
    "    \"\"\" \n",
    "    Input: \n",
    "        - class_y: list of class labels (0's and 1's)\n",
    "    \n",
    "    TODO: Compute the entropy for a list of classes\n",
    "    Example: entropy([0,0,0,1,1,1,1,1]) = 0.9544\n",
    "    \"\"\"\n",
    "    prob = np.array(list(Counter(class_y).values()))\n",
    "    prob = prob*1.0/len(class_y)   #Normalize to 1\n",
    "    entropy = np.sum(-prob*np.log2(prob))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wp7wEfZ82Owg"
   },
   "outputs": [],
   "source": [
    "def information_gain(previous_y, current_y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - previous_y : the distribution of original labels (0's and 1's)\n",
    "        - current_y  : the distribution of labels after splitting based on a particular\n",
    "                     split attribute and split value\n",
    "    \n",
    "    TODO: Compute and return the information gain from partitioning the previous_y labels into the current_y labels.\n",
    "    \n",
    "    Reference: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-s06/www/DTs.pdf \n",
    "\n",
    "    Example: previous_y = [0,0,0,1,1,1], current_y = [[0,0], [1,1,1,0]], info_gain = 0.4591\n",
    "    \"\"\" \n",
    "    H_prev = entropy(previous_y)\n",
    "    \n",
    "    left, right = current_y\n",
    "    assert len(previous_y) == len(left) + len(right)\n",
    "    p_left, p_right = len(left)*1.0/(len(previous_y)), len(right)*1.0/(len(previous_y))\n",
    "    H_left, H_right = entropy(left), entropy(right)\n",
    "    \n",
    "    info_gain = H_prev - (p_left*H_left + p_right*H_right)\n",
    "    \n",
    "    return info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.954434002924965\n",
      "0.4591479170272448\n"
     ]
    }
   ],
   "source": [
    "# TEST CASE\n",
    "test_class_y = [0,0,0,1,1,1,1,1]\n",
    "print(entropy(test_class_y))\n",
    "\n",
    "previous_y = [0,0,0,1,1,1]\n",
    "current_y = [[0,0], [1,1,1,0]] \n",
    "print(information_gain(previous_y, current_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-xDzI702jxv"
   },
   "source": [
    "### Build a simple desicion tree step by step [30pts]\n",
    "\n",
    "Now we will implement three functions to build a decision tree from the scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mBJIZFd2lx4"
   },
   "source": [
    "#### (1) partition_classes: [10pts]\n",
    "\n",
    "One of the basic operations is to split a tree on one attribute (features) with a specific value for that attribute.\n",
    "\n",
    "In partition_classes(), we split the data (X) and labels (y) based on the split feature and value - BINARY SPLIT.\n",
    "\n",
    "You will have to first check if the split attribute is numerical or categorical. If the split attribute is numeric, split_val should be a numerical value. For example, your split_val could be the mean of the values of split_attribute. If the split attribute is categorical, split_val should be one of the categories.   \n",
    "    \n",
    "You can perform the partition in the following way:\n",
    "   - Numeric Split Attribute:\n",
    "   \n",
    "       Split the data X into two lists(X_left and X_right) where the first list has all\n",
    "       the rows where the split attribute is less than or equal to the split value, and the \n",
    "       second list has all the rows where the split attribute is greater than the split \n",
    "       value. Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "    \n",
    "   - Categorical Split Attribute:\n",
    "   \n",
    "       Split the data X into two lists(X_left and X_right) where the first list has all \n",
    "       the rows where the split attribute is equal to the split value, and the second list\n",
    "       has all the rows where the split attribute is not equal to the split value.\n",
    "       Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-Ch02qB2oJm"
   },
   "outputs": [],
   "source": [
    "def partition_classes(X, y, split_attribute, split_val):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X               : (N,D) list containing all data attributes\n",
    "    - y               : a list of labels\n",
    "    - split_attribute : column index of the attribute to split on\n",
    "    - split_val       : either a numerical or categorical value to divide the split_attribute\n",
    "    \n",
    "    TODO: Partition the data(X) and labels(y) based on the split value - BINARY SPLIT.\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    X = [[3, 'aa', 10],                 y = [1,\n",
    "         [1, 'bb', 22],                      1,\n",
    "         [2, 'cc', 28],                      0,\n",
    "         [5, 'bb', 32],                      0,\n",
    "         [4, 'cc', 32]]                      1]\n",
    "    \n",
    "    Here, columns 0 and 2 represent numeric attributes, while column 1 is a categorical attribute.\n",
    "    \n",
    "    Consider the case where we call the function with split_attribute = 0 and split_val = 3 (mean of column 0)\n",
    "    Then we divide X into two lists - X_left, where column 0 is <= 3  and X_right, where column 0 is > 3.\n",
    "    \n",
    "    X_left = [[3, 'aa', 10],                 y_left = [1,\n",
    "              [1, 'bb', 22],                           1,\n",
    "              [2, 'cc', 28]]                           0]\n",
    "              \n",
    "    X_right = [[5, 'bb', 32],                y_right = [0,\n",
    "               [4, 'cc', 32]]                           1]\n",
    "\n",
    "    Consider another case where we call the function with split_attribute = 1 and split_val = 'bb'\n",
    "    Then we divide X into two lists, one where column 1 is 'bb', and the other where it is not 'bb'.\n",
    "        \n",
    "    X_left = [[1, 'bb', 22],                 y_left = [1,\n",
    "              [5, 'bb', 32]]                           0]\n",
    "              \n",
    "    X_right = [[3, 'aa', 10],                y_right = [1,\n",
    "               [2, 'cc', 28],                           0,\n",
    "               [4, 'cc', 32]]                           1]\n",
    "               \n",
    "    \"\"\"\n",
    "    X_left, X_right, y_left, y_right = [], [], [], []\n",
    "    \n",
    "    def is_left(X,ii, split_attribute, split_val):\n",
    "        if isinstance(split_val,numbers.Number):\n",
    "            return X[ii][split_attribute] <= split_val\n",
    "        else:\n",
    "            return X[ii][split_attribute] == split_val\n",
    "    \n",
    "    for ii in range(len(y)):\n",
    "        if is_left(X,ii, split_attribute, split_val):\n",
    "            X_left.append(X[ii])\n",
    "            y_left.append(y[ii])\n",
    "        else:\n",
    "            X_right.append(X[ii])\n",
    "            y_right.append(y[ii])\n",
    "\n",
    "    # Return in this order\n",
    "    return (X_left, X_right, y_left, y_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GpsToj_T2x42"
   },
   "source": [
    "#### (2) find_best_split [10pts]\n",
    "\n",
    "Given the data and labels, we need to find the order of splitting features, which is also the importance of the feature. For each attribute (feature), we need to calculate its optimal split value along with the corresponding information gain and then compare with all the features to find the optimal attribute to split.\n",
    "\n",
    "First, we specify an attribute. After computing the corresponding information gain of each value at this attribute list, we can get the optimal split value, which has the maximum information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nG3Def4L20re"
   },
   "outputs": [],
   "source": [
    "def find_best_split(X, y, split_attribute):\n",
    "    \"\"\"Inputs:\n",
    "        - X               : (N,D) list containing all data attributes\n",
    "        - y               : a list array of labels\n",
    "        - split_attribute : Column of X on which to split\n",
    "    \n",
    "    TODO: Compute and return the optimal split value for a given attribute, along with the corresponding information gain\n",
    "    \n",
    "    Note: You will need the functions information_gain and partition_classes to write this function\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "        X = [[3, 'aa', 10],                 y = [1,\n",
    "             [1, 'bb', 22],                      1,\n",
    "             [2, 'cc', 28],                      0,\n",
    "             [5, 'bb', 32],                      0,\n",
    "             [4, 'cc', 32]]                      1]\n",
    "    \n",
    "        split_attribute = 0\n",
    "        \n",
    "        Starting entropy: 0.971\n",
    "        \n",
    "        Calculate information gain at splits:\n",
    "           split = 1  -->  info_gain = 0.17\n",
    "           split = 2  -->  info_gain = 0.02\n",
    "           split = 3  -->  info_gain = 0.02\n",
    "           split = 4  -->  info_gain = 0.32\n",
    "           split = 5  -->  info_gain = 0.\n",
    "        \n",
    "       best_split_val = 4; info_gain = .32;\n",
    "    \"\"\"\n",
    "    best_split_val, max_IG = X[0][split_attribute], 0\n",
    "    \n",
    "    possible_vals = set([X[ii][split_attribute] for ii in range(len(X))])\n",
    "    \n",
    "    for split_val in possible_vals:\n",
    "        X_left, X_right, y_left, y_right = partition_classes(X, y, split_attribute, split_val)\n",
    "        current_IG = information_gain(y, [y_left, y_right])\n",
    "        \n",
    "        if len(y_left) == 0 or len(y_right) == 0:\n",
    "            continue\n",
    "        \n",
    "        if current_IG>max_IG:\n",
    "            best_split_val, max_IG = split_val, current_IG\n",
    "    \n",
    "    return best_split_val, max_IG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8eM_fLu3GN9"
   },
   "source": [
    "#### (3)  find_best_feature [10pts]\n",
    "\n",
    "Based on the above functions, we can find the most important feature that we will split first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "253k0w6Y3ISy"
   },
   "outputs": [],
   "source": [
    "def find_best_feature(X, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - X: (N,D) list containing all data attributes\n",
    "        - y : a list of labels\n",
    "    \n",
    "    TODO: Compute and return the optimal attribute to split on and optimal splitting value\n",
    "    \n",
    "    Note: If two features tie, choose one of them at random\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "        X = [[3, 'aa', 10],                 y = [1,\n",
    "             [1, 'bb', 22],                      1,\n",
    "             [2, 'cc', 28],                      0,\n",
    "             [5, 'bb', 32],                      0,\n",
    "             [4, 'cc', 32]]                      1]\n",
    "    \n",
    "        split_attribute = 0\n",
    "        \n",
    "        Starting entropy: 0.971\n",
    "        \n",
    "        Calculate information gain at splits:\n",
    "           feature 0:  -->  info_gain = 0.32\n",
    "           feature 1:  -->  info_gain = 0.17\n",
    "           feature 2:  -->  info_gain = 0.42\n",
    "        \n",
    "       best_split_feature = 2; best_split_val = 22;\n",
    "    \"\"\"\n",
    "    best_feature, best_split_val, best_IG = 0, X[0][0], 0\n",
    "    \n",
    "    for split_feature in range(len(X[0])):\n",
    "        current_best_split_val, current_max_IG = find_best_split(X, y, split_feature)\n",
    "    \n",
    "        if current_max_IG>best_IG:\n",
    "            best_feature, best_split_val, best_IG = split_feature, current_best_split_val, current_max_IG\n",
    "            \n",
    "    return best_feature, best_split_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[3, 'aa', 10], [1, 'bb', 22], [2, 'cc', 28]], [[5, 'bb', 32], [4, 'cc', 32]], [1, 1, 0], [0, 1])\n",
      "([[1, 'bb', 22], [5, 'bb', 32]], [[3, 'aa', 10], [2, 'cc', 28], [4, 'cc', 32]], [1, 0], [1, 0, 1])\n",
      "best_split_val: 4 info_gain: 0.3219280948873623\n",
      "best_split_feature: 2 best_split_val: 22\n"
     ]
    }
   ],
   "source": [
    "# TEST CASE\n",
    "test_X = [[3, 'aa', 10],[1, 'bb', 22],[2, 'cc', 28],[5, 'bb', 32],[4, 'cc', 32]]\n",
    "test_y = [1,1,0,0,1]\n",
    "print(partition_classes(test_X, test_y, 0, 3))\n",
    "print(partition_classes(test_X, test_y, 1, 'bb'))\n",
    "\n",
    "split_attribute = 0\n",
    "best_split_val, info_gain = find_best_split(test_X, test_y, split_attribute)\n",
    "print(\"best_split_val:\", best_split_val, \"info_gain:\", info_gain)\n",
    "\n",
    "best_feature, best_split_val = find_best_feature(test_X, test_y)\n",
    "print(\"best_split_feature:\", best_feature, \"best_split_val:\", best_split_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ayv_tqnyxMb0"
   },
   "source": [
    "# Part 2: Decision Tree [30 pts]\n",
    "\n",
    "In this part, you will train the decision tree and then use this tree to make predictions.\n",
    "\n",
    "For starters, let's consider the following algorithm (a variation of C4.5) for the construction of a decision tree:\n",
    "\n",
    "    1) Check for base cases: \n",
    "         a)If all elements of a list are of the same class, return a leaf node with the appropriate class label.\n",
    "         b)If a specified depth limit is reached, return a leaf labeled with the most frequent class.\n",
    "\n",
    "    2) For each attribute alpha: evaluate the normalized information gain gained by splitting on alpha\n",
    "\n",
    "    3) Let alpha_best be the attribute with the highest normalized information gain\n",
    "\n",
    "    4) Create a decision node that splits on alpha_best\n",
    "\n",
    "    5) Recur on the sublists obtained by splitting on alpha_best, and add those nodes as children of node\n",
    "\n",
    "You need to finish the following three functions. In the \\__init\\__(), we initialize the tree as an empty dictionary or list. In the fit(), we fit a decision tree (self.tree) using the the features X and labels y. You could see the dataset (hw4data.csv).  In the predict(), write a function to produce classifications for a list of features once your decision tree has been build.\n",
    "\n",
    "[reference: http://www.cs.cmu.edu/~cga/ai-course/dtree.pdf]\n",
    "\n",
    "**Hint:** We need to use the recursion, so we need to select a stop condition which showed in step 1). Meanwhile, you could also define a max depth to stop. Here, we provide a simple recursion example for the tree traversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "14\n",
      "10\n",
      "19\n",
      "35\n",
      "31\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "class Node(): \n",
    "    def __init__(self, data, left, right):                                       \n",
    "        self.data = data                                                            \n",
    "        self.left = left                                                            \n",
    "        self.right = right\n",
    "\n",
    "class BTree:                                                                        \n",
    "    def __init__(self):                                                             \n",
    "        self.root = None                                                                                                                  \n",
    "\n",
    "    def insert(self, data):   #insert nodes                                                 \n",
    "        r = self.root                                                            \n",
    "        if r is None:                                                            \n",
    "            self.root = Node(data, None, None)                                  \n",
    "            return                                                                  \n",
    "        while True:                                                                 \n",
    "            # if the node is smaller than the root                                                  \n",
    "            if r.data > data:                                                       \n",
    "                if r.left is None:                                                  \n",
    "                    r.left = Node(data, None, None)                             \n",
    "                    break                                                           \n",
    "                else:                                                               \n",
    "                    r = r.left                                                      \n",
    "            else:                                                                   \n",
    "            # if the node is larger than the root                             \n",
    "                if r.right is None:                                                 \n",
    "                    r.right = Node(data, None, None)                            \n",
    "                    break                                                           \n",
    "                else:                                                               \n",
    "                    r = r.right \n",
    "    def preorder(self, root):                                                      \n",
    "            if root is None:                                                            \n",
    "                return                                                                  \n",
    "            else:                                                                                                                                  \n",
    "                print(root.data)                                                        \n",
    "                self.preorder(root.left)                          \n",
    "                self.preorder(root.right)  \n",
    "            \n",
    "if __name__ == '__main__':                                                          \n",
    "    bt = BTree()\n",
    "    L = [27, 14, 10, 19, 35, 31, 42] #the first one is the node\n",
    "    for i in L:                                      \n",
    "        bt.insert(i) \n",
    "    # The tree will have three layers and from the top to the bottom, it will look like [27, [14,35], [10,19,31,42]]\n",
    "    bt.preorder(bt.root)\n",
    "    # Pre-order traversal\n",
    "    # Reference: https://en.wikipedia.org/wiki/Tree_traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2Cwef24xgtT"
   },
   "outputs": [],
   "source": [
    "class MyDecisionTree(object):\n",
    "    def __init__(self, max_depth = None):\n",
    "        \"\"\"\n",
    "        TODO: Initializing the tree as an empty dictionary or list, as preferred.\n",
    "        \n",
    "        For example: self.tree = [] or self.tree = {}\n",
    "        \"\"\"\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        \n",
    "        self.best_feature = None\n",
    "        self.best_split_val = None\n",
    "        \n",
    "        self.is_leaf = False\n",
    "        self.leaf_value = None\n",
    "        \n",
    "        self.max_depth = max_depth if max_depth != None else float('inf')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        TODO: Train the decision tree (self.tree) using the the sample X and labels y.\n",
    "        \n",
    "        NOTE: You will have to make use of the functions in utils.py to train the tree.\n",
    "        One possible way of implementing the tree: Each node in self.tree could be in the form of a dictionary:\n",
    "        https://docs.python.org/2/library/stdtypes.html#mapping-types-dict\n",
    "        \n",
    "        For example, a non-leaf node with two children can have a 'left' key and  a  'right' key. \n",
    "        You can add more keys which might help in classification (eg. split attribute and split value)\n",
    "        \"\"\"\n",
    "        if self.max_depth <= 0:\n",
    "            self.is_leaf = True\n",
    "            self.leaf_value = Counter(y).most_common(1)[0][0]\n",
    "            return \n",
    "            \n",
    "        if entropy(y) == 0:\n",
    "            self.is_leaf = True\n",
    "            self.leaf_value = y[0]\n",
    "            return\n",
    "        \n",
    "        best_feature, best_split_val = find_best_feature(X, y)\n",
    "        self.best_feature = best_feature\n",
    "        self.best_split_val = best_split_val\n",
    "        \n",
    "        X_left, X_right, y_left, y_right = partition_classes(X, y, best_feature, best_split_val)\n",
    "        \n",
    "        assert len(y_left) != 0\n",
    "        self.left = MyDecisionTree(max_depth = self.max_depth-1)\n",
    "        self.left.fit(X_left, y_left)\n",
    "\n",
    "        assert len(y_right) != 0\n",
    "        self.right = MyDecisionTree(max_depth = self.max_depth-1)\n",
    "        self.right.fit(X_right, y_right)\n",
    "\n",
    "\n",
    "    def predict(self, record):\n",
    "        \"\"\"\n",
    "        TODO: classify a sample in test data set using self.tree and return the predicted label\n",
    "        \"\"\"\n",
    "        if self.is_leaf:\n",
    "            return self.leaf_value\n",
    "        \n",
    "        value = record[self.best_feature]\n",
    "        check_left = (value <= self.best_split_val) if isinstance(value,numbers.Number) else (value == self.best_split_val)\n",
    "\n",
    "        if check_left:\n",
    "            return self.left.predict(record)\n",
    "        else:\n",
    "            return self.right.predict(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZwVF7SGy8Cy"
   },
   "source": [
    "Now, let us use the Decision Tree to build a classifier and then to make predictions.\n",
    "\n",
    "The accuracy may be difference since it will depends on the stop condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0IvbriYy5yN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading hw4-data\n",
      "[30, 'blue-collar', 'married', 'basic.9y', 'no', 'yes', 'no', 'cellular', 'may', 'fri', 487, 2, 999, 0, 'nonexistent', -1.8, 92.893, -46.2, 1.313, 5099.1]\n",
      "inf\n",
      "fitting the decision tree\n",
      "accuracy: 0.8746\n"
     ]
    }
   ],
   "source": [
    "def DecisionTreeEvalution(depth = None):\n",
    "    X = list()\n",
    "    y = list()\n",
    "    numerical_cols = set([0,10,11,12,13,15,16,17,18,19,20]) # indices of numeric attributes (columns)\n",
    "    training_num = 2500\n",
    "    # Loading data set\n",
    "    print(\"reading hw4-data\")\n",
    "    with open(\"hw4-data.csv\") as f:\n",
    "        next(f, None)\n",
    "\n",
    "        for line in csv.reader(f, delimiter=\",\"):\n",
    "            xline = []\n",
    "            for i in range(len(line)):\n",
    "                if i in numerical_cols:\n",
    "                    xline.append(ast.literal_eval(line[i]))\n",
    "                else:\n",
    "                    xline.append(line[i])\n",
    "\n",
    "            X.append(xline[:-1])\n",
    "            y.append(xline[-1])\n",
    "    \n",
    "    print(X[0]) # print a data sample\n",
    "    \n",
    "    # Initializing a decision tree.\n",
    "    dt = MyDecisionTree(max_depth = depth)\n",
    "    print(dt.max_depth)\n",
    "\n",
    "    # Building a tree\n",
    "    print(\"fitting the decision tree\")\n",
    "    dt.fit(X[:training_num], y[:training_num])\n",
    "\n",
    "    # Make predictions\n",
    "    # For each test sample X, use our fitting dt classifer to predict\n",
    "    y_predicted = []\n",
    "    for record in X[training_num:]: \n",
    "        y_predicted.append(dt.predict(record))\n",
    "\n",
    "    # Comparing predicted and true labels\n",
    "    results = [prediction == truth for prediction, truth in zip(y_predicted, y[training_num:])]\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True)) / float(len(results))\n",
    "    print(\"accuracy: %.4f\" % accuracy)\n",
    "    \n",
    "DecisionTreeEvalution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading hw4-data\n",
      "[30, 'blue-collar', 'married', 'basic.9y', 'no', 'yes', 'no', 'cellular', 'may', 'fri', 487, 2, 999, 0, 'nonexistent', -1.8, 92.893, -46.2, 1.313, 5099.1]\n",
      "2\n",
      "fitting the decision tree\n",
      "accuracy: 0.8987\n"
     ]
    }
   ],
   "source": [
    "DecisionTreeEvalution(depth = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aGIzP6dtY-Y"
   },
   "source": [
    "## Part 3: Random Forests [30pts]\n",
    "\n",
    "The decision boundaries drawn by decision trees are very sharp, and fitting a decision tree of unbounded depth to a list of examples almost inevitably leads to **overfitting**. In an attempt to decrease the variance of our classifier we're going to use a technique called 'Bootstrap Aggregating' (often abbreviated 'bagging').\n",
    "\n",
    "A Random Forest is a collection of decision trees, built as follows:\n",
    "\n",
    "1) For every tree we're going to build:\n",
    "\n",
    "    a) Subsample the examples provided (with replacement) in accordance with a provided example subsampling rate.\n",
    "    \n",
    "    b) From the sample in a), choose attributes at random to learn on (in accordance with a provided attribute subsampling rate)\n",
    "    \n",
    "    c) Fit a decision tree to the subsample of data we've chosen (to a certain depth)\n",
    "    \n",
    "Classification for a random forest is then done by taking a majority vote of the classifications yielded by each tree in the forest after it classifies an example.\n",
    "\n",
    "In RandomForests Class, \n",
    "1. X is assumed to be a matrix with n rows and d columns where n is the\n",
    "number of total records and d is the number of features of each record. \n",
    "\n",
    "2. y is assumed to be a vector of labels of length n.\n",
    "\n",
    "3. XX is similar to X, except that XX also contains the data label for each\n",
    "record.\n",
    "\n",
    "**NOTE:**\n",
    "1. Lookout for TODOs for the parts that needs to be implemented.\n",
    "2. Do NOT change the signature of the given functions.\n",
    "3. Do NOT change any part of the randomForestClassifier function APART from the forest_size parameter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6n8GGVU7tYGh"
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "\"\"\"\n",
    "NOTE: We import the tree from sklearn so that even if you can not build a decision tree sucessfully, \n",
    "you could still finish the random forest classifer.\n",
    "\n",
    "You are welcome to try to use your own decision tree MyDecisionTree() to finish random forest as well,\n",
    "but for grading we will use the sklearn tree provided here.\n",
    "\"\"\"\n",
    "\n",
    "class RandomForest(object):\n",
    "#     num_trees = 0\n",
    "#     decision_trees = []\n",
    "\n",
    "    # the bootstrapping datasets for trees\n",
    "    # bootstraps_datasets is a list of lists, where each list in bootstraps_datasets is a bootstrapped dataset.\n",
    "#     bootstraps_datasets = []\n",
    "\n",
    "    # the true class labels, corresponding to records in the bootstrapping datasets\n",
    "    # bootstraps_labels is a list of lists, where the 'i'th list contains the labels corresponding to samples in the 'i'th bootstrapped dataset.\n",
    "#     \n",
    "\n",
    "    def __init__(self, num_trees = 10, seed = 1):\n",
    "        # Initialization done here\n",
    "        self.num_trees = num_trees\n",
    "        self.decision_trees = [tree.DecisionTreeClassifier() for i in range(num_trees)] # from sklearn\n",
    "        self.bootstraps_datasets = []\n",
    "        self.bootstraps_labels = []\n",
    "        np.random.seed(seed=seed)\n",
    "\n",
    "    def _bootstrapping(self, XX, n):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Create a sample dataset of size n by randomly sampling with replacement from the original dataset XX.\n",
    "        Note that you would also need to record the corresponding class labels for the sampled records for training purposes. \n",
    "        Reference: https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n",
    "        \"\"\"\n",
    "         \n",
    "        samples = [] # sampled dataset\n",
    "        labels = []  # class labels for the sampled records\n",
    "        \n",
    "        N = len(XX)\n",
    "        indexes = list(set(np.random.choice(np.arange(N),N-1)))\n",
    "        for ii in indexes:\n",
    "            samples.append(XX[ii][:-1])\n",
    "            labels.append(XX[ii][-1])\n",
    "        \n",
    "        return (samples, labels)\n",
    "        \n",
    "\n",
    "    def bootstrapping(self, XX):\n",
    "        # Initializing the bootstap datasets for each tree\n",
    "        for i in range(self.num_trees):\n",
    "            data_sample, data_label = self._bootstrapping(XX, len(XX))\n",
    "            self.bootstraps_datasets.append(data_sample)\n",
    "            self.bootstraps_labels.append(data_label)\n",
    "\n",
    "\n",
    "    def fitting(self):\n",
    "        \"\"\"\n",
    "        TODO: Train `num_trees` decision trees using the bootstraps datasets and labels by calling the sklearn DecisionTree class.\n",
    "        \"\"\"\n",
    "        for ii, (X, y) in enumerate(zip(self.bootstraps_datasets, self.bootstraps_labels)):\n",
    "            self.decision_trees[ii].fit(X,y)\n",
    "\n",
    "\n",
    "    def voting(self, X):\n",
    "        y = []\n",
    "\n",
    "        for record in X:\n",
    "            # Following steps have been performed here:\n",
    "            #   1. Find the set of trees that consider the record as an out-of-bag sample.\n",
    "            #   2. Predict the label using each of the above found trees.\n",
    "            #   3. Use majority vote to find the final label for this record.\n",
    "            votes = []\n",
    "            for i in range(len(self.bootstraps_datasets)):\n",
    "                dataset = self.bootstraps_datasets[i]\n",
    "                if record not in dataset:\n",
    "                    OOB_tree = self.decision_trees[i]\n",
    "                    effective_vote = OOB_tree.predict([record])\n",
    "                    votes.append(effective_vote[0])\n",
    "\n",
    "\n",
    "            counts = np.bincount(votes)\n",
    "            \n",
    "            if len(counts) == 0:\n",
    "            #  Special case \n",
    "                #  Handle the case where the record is not an out-of-bag sample for any of the trees.\n",
    "                y = np.append(y, 0)\n",
    "            else:\n",
    "                y = np.append(y, np.argmax(counts))\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "AC1-lWuct2wj",
    "outputId": "006bf714-6c0a-4d3e-f695-c1b326f0670d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data.\n",
      "Creating the bootstrap datasets.\n",
      "Fitting the forest.\n",
      "Accuracy: 0.8988\n",
      "OOB estimate: 0.1012\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE: Do not change this function apart from the forest_size parameter. \n",
    "The accuracy will be a little different due to the random bootstrapping sampling.\n",
    "\"\"\"\n",
    "def randomForestClassifier():\n",
    "    X = list()\n",
    "    y = list()\n",
    "    XX = list()  # Contains data features and data labels\n",
    "    numerical_cols = set([0,10,11,12,13,15,16,17,18,19,20]) # indices of numeric attributes (columns)\n",
    "    \n",
    "    # Loading data set\n",
    "    print(\"Reading the data.\")\n",
    "    with open(\"hw4-data.csv\") as f:\n",
    "        next(f, None)\n",
    "\n",
    "        for line in csv.reader(f, delimiter=\",\"):\n",
    "            xline = []\n",
    "            for i in range(len(line)):\n",
    "                if i in numerical_cols:\n",
    "                    xline.append(ast.literal_eval(line[i]))\n",
    "\n",
    "            X.append(xline[:-1])\n",
    "            y.append(xline[-1])\n",
    "            XX.append(xline[:])\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: Initialize forest_size according to your implementation\n",
    "    \"\"\"\n",
    "    # Minimum forest_size should be 10\n",
    "    forest_size = 10\n",
    "    \n",
    "    # Initializing a random forest.\n",
    "    randomForest = RandomForest(forest_size)\n",
    "\n",
    "    # Creating the bootstrapping datasets\n",
    "    print(\"Creating the bootstrap datasets.\")\n",
    "    randomForest.bootstrapping(XX)\n",
    "\n",
    "    # Building trees in the forest\n",
    "    print(\"Fitting the forest.\")\n",
    "    randomForest.fitting()\n",
    "\n",
    "    # Calculating an unbiased error estimation of the random forest\n",
    "    # based on out-of-bag (OOB) error estimate.\n",
    "    y_predicted = randomForest.voting(X)\n",
    "\n",
    "    # Comparing predicted and true labels\n",
    "    results = [prediction == truth for prediction, truth in zip(y_predicted, y)]\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True)) / float(len(results))\n",
    "\n",
    "    print(\"Accuracy: %.4f\" % accuracy)\n",
    "    print(\"OOB estimate: %.4f\" % (1-accuracy))\n",
    "\n",
    "randomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MpRZNRBbzvOX"
   },
   "source": [
    "Bouns Part : Challenge! \n",
    "-------\n",
    "This part of the assignment is an opportunity to get extra credit for your final grade.  To do so, we will be holding a competition to see who can write the best classifier to make predictions on a private research dataset.  Your classifier should follow the standard sklearn format with .fit() and .predict() methods.  This problem will not give you points on this assignment, but will count towards your final grade as follows:\n",
    "\n",
    "First place:  +3% on your final grade\n",
    "\n",
    "Second place: +2% on your final grade\n",
    "\n",
    "Third place:  +1% on your final grade\n",
    "\n",
    "If there are ties in accuracy, winner will be determined by submission time.\n",
    "\n",
    "\n",
    "You've been provided with a sample of data from a research dataset in 'challenge_data.pickle'. It is serialized as a tuple of (features, classes). I have reserved a part of the dataset for testing. The classifier that performs most accurately on the holdout set wins (so optimize for accuracy). \n",
    "\n",
    "As a minimum bar for getting extra credit, you'll need to get at least an average accuracy of 80% on the data you have, and at least an average accuracy of 60% on the holdout set.\n",
    "\n",
    "Other rules:\n",
    "* You are NOT allowed to import any pre-built classifiers (i.e. sklearn, xgboost, lightgbm, statsmodels, etc)\n",
    "* You can import utilities from the following libraries to build neural networks: \n",
    "    * keras \n",
    "    * tensorflow \n",
    "    * pytorch\n",
    "* You are allowed to enter classifiers you have built in this or other assigments (though you'll probably want to improve them a bit)\n",
    "* You may add any feature engineering you wish to your .fit() method to improve your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y51v-cqPzvig"
   },
   "outputs": [],
   "source": [
    "class ChallengeClassifier():\n",
    "    \n",
    "    def __init__(self, batch_size = 25, epoches = 30):\n",
    "        # initialize whatever parameters you may need here-\n",
    "        # this method will be called without parameters \n",
    "        # so if you add any to make parameter sweeps easier, provide defaults\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.epoches = epoches\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dense(500,activation='relu'))\n",
    "        self.model.add(Dropout(0.1))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dense(300,activation='relu'))\n",
    "        #self.model.add(Dropout(0.1))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "    def fit(self, features, classes, epoches = 10):\n",
    "        # fit your model to the provided features\n",
    "        e = epoches if epoches else self.epoches\n",
    "        self.model.fit(features, classes,\n",
    "          batch_size=self.batch_size,\n",
    "          epochs=e,\n",
    "          verbose = False)\n",
    "        \n",
    "    def predict(self, features):\n",
    "        # classify each feature in features as either 0 or 1.\n",
    "        y_pred = 1.0*(self.model.predict(features)>0.5)\n",
    "        return y_pred.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('hw4-data.csv')\n",
    "df =  df.select_dtypes(exclude='object')\n",
    "\n",
    "X, y = df[df.columns[:-1]], df[df.columns[-1]].values\n",
    "training_num = 2500\n",
    "X_train, X_test = X.iloc[:training_num].values, X.iloc[training_num:].values\n",
    "y_train, y_test = y[:training_num], y[training_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChallengeClassifier(batch_size = 25, epoches = 10)\n",
    "model.fit(X_train,y_train, epoches = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.7350216182829\n",
      "90.36442248301421\n",
      "91.59975293390981\n",
      "90.61148857319333\n",
      "90.92032118591723\n",
      "90.98208770846202\n",
      "90.48795552810377\n",
      "90.79678814082767\n",
      "90.79678814082767\n",
      "90.54972205064855\n",
      "90.85855466337244\n",
      "90.85855466337244\n",
      "90.85855466337244\n",
      "90.54972205064855\n",
      "90.79678814082767\n",
      "90.79678814082767\n",
      "90.24088943792464\n",
      "89.87029030265596\n",
      "90.48795552810377\n",
      "90.0555898702903\n",
      "Best Score: 91.59975293390981\n"
     ]
    }
   ],
   "source": [
    "best_score = -1\n",
    "for _ in range(20):\n",
    "    model.fit(X_train,y_train, epoches = 1)\n",
    "    score = np.mean(model.predict(X_test) == y_test)*100\n",
    "    print(score)\n",
    "    best_score = max(score, best_score)\n",
    "print('Best Score:', best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best prediction score is 91.599% (third one from the top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
